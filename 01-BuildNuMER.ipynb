{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) Data Preparation\n",
    "\n",
    "This notebook demonstrates the process of preparing data for Named Entity Recognition (NER) using the `datasets` library. The workflow includes:\n",
    "\n",
    "1. **Loading the Dataset**: The dataset is loaded using the `load_dataset` function from the `datasets` library.\n",
    "2. **Normalization and Parsing**: Functions are defined to normalize tokens, find sublist indices, and parse examples to extract entities and their labels.\n",
    "3. **Dataset Processing**: The dataset is processed to convert text and annotations into a format suitable for NER tasks.\n",
    "4. **Data Splitting**: The processed data is split into training and testing sets.\n",
    "5. **Saving the Data**: The training and testing sets are saved as JSON files for further use.\n",
    "\n",
    "The notebook ensures that the data is properly tokenized and annotated, making it ready for training GliNER models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import ast\n",
    "import string\n",
    "\n",
    "def normalize_token(token):\n",
    "    # Remove punctuation and lowercase the token.\n",
    "    return token.strip(string.punctuation).lower()\n",
    "\n",
    "def find_sublist_indices_norm(main_list, sub_list):\n",
    "    \"\"\"\n",
    "    Return (start_index, end_index) if the normalized sub_list occurs \n",
    "    consecutively in the normalized main_list, else return None.\n",
    "    \"\"\"\n",
    "    norm_main = [normalize_token(t) for t in main_list]\n",
    "    norm_sub = [normalize_token(t) for t in sub_list]\n",
    "    n = len(norm_main)\n",
    "    m = len(norm_sub)\n",
    "    for i in range(n - m + 1):\n",
    "        if norm_main[i : i + m] == norm_sub:\n",
    "            return i, i + m - 1\n",
    "    return None\n",
    "\n",
    "def parse_example(example):\n",
    "    # Split the input text by whitespace.\n",
    "    text = example[\"input\"]\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Convert the output field (a string representation of a list) to an actual list.\n",
    "    try:\n",
    "        annotations = ast.literal_eval(example[\"output\"])\n",
    "    except Exception as e:\n",
    "        annotations = []\n",
    "    \n",
    "    spans = []\n",
    "    for ann in annotations:\n",
    "        # Each annotation is expected to have the format: \"Entity text <> Label\"\n",
    "        if \"<>\" not in ann:\n",
    "            continue\n",
    "        ent_text, ent_label = [x.strip() for x in ann.split(\"<>\")]\n",
    "        # Split the entity text by space.\n",
    "        ent_tokens = ent_text.split()\n",
    "        indices = find_sublist_indices_norm(tokens, ent_tokens)\n",
    "        if indices is not None:\n",
    "            start, end = indices\n",
    "            spans.append([start, end, ent_label])\n",
    "    \n",
    "    # Add new keys as expected by GLiNER.\n",
    "    example[\"tokenized_text\"] = tokens\n",
    "    example[\"ner\"] = spans\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 1000000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3) Load your local CSV with the 'datasets' library\n",
    "#    Suppose your CSV has columns \"input\" and \"output\"\n",
    "ds = load_dataset(\n",
    "    \"numind/NuNER\",\n",
    "    split=\"entity\",  # 'train' if it's a single file\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'State University of New York Press, 1997.', 'output': \"['State University of New York Press <> Publisher']\"}\n"
     ]
    }
   ],
   "source": [
    "example = ds[0]\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'State University of New York Press, 1997.',\n",
       " 'output': \"['State University of New York Press <> Publisher']\",\n",
       " 'tokenized_text': ['State',\n",
       "  'University',\n",
       "  'of',\n",
       "  'New',\n",
       "  'York',\n",
       "  'Press,',\n",
       "  '1997.'],\n",
       " 'ner': [[0, 5, 'Publisher']]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_example(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'According to the 2015 census, it has a population of 70,757 people.',\n",
       " 'output': \"['2015 census <> Time', '70,757 people <> Quantity']\",\n",
       " 'tokenized_text': ['According',\n",
       "  'to',\n",
       "  'the',\n",
       "  '2015',\n",
       "  'census,',\n",
       "  'it',\n",
       "  'has',\n",
       "  'a',\n",
       "  'population',\n",
       "  'of',\n",
       "  '70,757',\n",
       "  'people.'],\n",
       " 'ner': [[3, 4, 'Time'], [10, 11, 'Quantity']]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_example(ds[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'This, of course, makes the new Senator worthy of closer inspection in our latest edition of Watch of the Week.',\n",
       " 'output': \"['Senator <> Political figure', 'Watch of the Week <> Media feature']\",\n",
       " 'tokenized_text': ['This,',\n",
       "  'of',\n",
       "  'course,',\n",
       "  'makes',\n",
       "  'the',\n",
       "  'new',\n",
       "  'Senator',\n",
       "  'worthy',\n",
       "  'of',\n",
       "  'closer',\n",
       "  'inspection',\n",
       "  'in',\n",
       "  'our',\n",
       "  'latest',\n",
       "  'edition',\n",
       "  'of',\n",
       "  'Watch',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Week.'],\n",
       " 'ner': [[6, 6, 'Political figure'], [16, 19, 'Media feature']]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_example(ds[10987])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [02:13<00:00, 7516.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "gliner_data = [\n",
    "    parse_example(example) for example in tqdm(ds) if example[\"input\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999997"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gliner_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(41)\n",
    "\n",
    "random.shuffle(gliner_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(899997, 100000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = gliner_data[: int(len(gliner_data) * .9)]\n",
    "test = gliner_data[int(len(gliner_data) * .9): ]\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/data/numer/train.json\", \"w\") as file:\n",
    "    json.dump(train, file)\n",
    "    \n",
    "with open(\"data/data/numer/test.json\", \"w\") as file:\n",
    "    json.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
